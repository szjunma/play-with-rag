{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval augmented generation using `llama_index` with local llm and embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jun/miniforge3/envs/rag/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "llama_model_loader: loaded meta data with 18 key-value pairs and 196 tensors from gpt4all-falcon/gpt4all-falcon-newbpe-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = falcon\n",
      "llama_model_loader: - kv   1:                               general.name str              = Falcon\n",
      "llama_model_loader: - kv   2:                      falcon.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                  falcon.tensor_data_layout str              = jploski\n",
      "llama_model_loader: - kv   4:                    falcon.embedding_length u32              = 4544\n",
      "llama_model_loader: - kv   5:                 falcon.feed_forward_length u32              = 18176\n",
      "llama_model_loader: - kv   6:                         falcon.block_count u32              = 32\n",
      "llama_model_loader: - kv   7:                falcon.attention.head_count u32              = 71\n",
      "llama_model_loader: - kv   8:             falcon.attention.head_count_kv u32              = 1\n",
      "llama_model_loader: - kv   9:        falcon.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,65024]   = [\">>TITLE<<\", \">>ABSTRACT<<\", \">>INTR...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,65024]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,65024]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,64784]   = [\"Ġ t\", \"Ġ a\", \"i n\", \"h e\", \"r e\",...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 11\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   66 tensors\n",
      "llama_model_loader: - type q4_0:  129 tensors\n",
      "llama_model_loader: - type q8_0:    1 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 12/65024 vs 0/65024 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = falcon\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 65024\n",
      "llm_load_print_meta: n_merges         = 64784\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4544\n",
      "llm_load_print_meta: n_head           = 71\n",
      "llm_load_print_meta: n_head_kv        = 1\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 71\n",
      "llm_load_print_meta: n_embd_k_gqa     = 64\n",
      "llm_load_print_meta: n_embd_v_gqa     = 64\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 18176\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 7.22 B\n",
      "llm_load_print_meta: model size       = 3.92 GiB (4.66 BPW) \n",
      "llm_load_print_meta: general.name     = Falcon\n",
      "llm_load_print_meta: BOS token        = 11 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 11 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 138 'Ä'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  3854.98 MiB, ( 3855.05 / 10922.67)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   158.50 MiB\n",
      "llm_load_tensors:      Metal buffer size =  3854.98 MiB\n",
      "...................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3200\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Pro\n",
      "ggml_metal_init: picking default device: Apple M1 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M1 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    25.00 MiB, ( 3881.86 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =    25.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   25.00 MiB, K (f16):   12.50 MiB, V (f16):   12.50 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =   127.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   494.64 MiB, ( 4376.50 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   494.63 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    15.13 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1093\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.eos_token_id': '11', 'tokenizer.ggml.model': 'gpt2', 'falcon.attention.layer_norm_epsilon': '0.000010', 'general.file_type': '2', 'falcon.attention.head_count_kv': '1', 'falcon.attention.head_count': '71', 'falcon.block_count': '32', 'falcon.feed_forward_length': '18176', 'falcon.embedding_length': '4544', 'falcon.tensor_data_layout': 'jploski', 'falcon.context_length': '2048', 'general.name': 'Falcon', 'general.architecture': 'falcon'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# define llm model\n",
    "Settings.llm = LlamaCPP(\n",
    "    model_path=\"gpt4all-falcon/gpt4all-falcon-newbpe-q4_0.gguf\",\n",
    "    context_window=3200,\n",
    "    max_new_tokens=256,\n",
    "    model_kwargs={'n_gpu_layers': -1},\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# define embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"UAE-Large-V1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# use tokenizer from defined llm model\n",
    "Settings.tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"gpt4all-falcon\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# load data and build index\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query your data\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8969.69 ms\n",
      "llama_print_timings:      sample time =      38.91 ms /   143 runs   (    0.27 ms per token,  3674.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13088.30 ms /  1998 tokens (    6.55 ms per token,   152.66 tokens per second)\n",
      "llama_print_timings:        eval time =    7948.49 ms /   142 runs   (   55.98 ms per token,    17.87 tokens per second)\n",
      "llama_print_timings:       total time =   22166.90 ms /  2140 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Silicon dioxide can be deposited using a combination of silicon precursor gasses like dichlorosilane or silane and oxygen precursors, typically at pressures from a few millitorr to a few torr. Plasma-deposited silicon nitride, formed from silane and ammonia or nitrogen, is also widely used, although it is important to note that it is not possible to deposit a pure nitride in this fashion. Plasma nitrides always contain a large amount of hydrogen, which can be bonded to silicon (Si-H) or nitrogen (Si-NH); this hydrogen has an important influence on IR and UV absorption, stability, mechanical stress, and electrical conductivity.\"\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How can silicon dioxide be deposited?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same llm model but without added information, we can ask the same question. The answer is more general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8969.69 ms\n",
      "llama_print_timings:      sample time =      44.68 ms /   241 runs   (    0.19 ms per token,  5393.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    9202.00 ms /   241 runs   (   38.18 ms per token,    26.19 tokens per second)\n",
      "llama_print_timings:       total time =   10349.91 ms /   242 tokens\n"
     ]
    }
   ],
   "source": [
    "# responce without RAG\n",
    "llm = Settings.llm\n",
    "res = llm.complete(\"How can silicon dioxide be deposited?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Silicon dioxide, also known as silica, can be deposited through various methods such as:\n",
      "\n",
      "1. Chemical vapor deposition (CVD): In this method, a gas containing silicon and oxygen is introduced into a chamber where it reacts with the surface of a substrate to form a thin film of silicon dioxide.\n",
      "\n",
      "2. Physical vapor deposition (PVD): In this method, a plasma of silicon and oxygen is generated in a vacuum chamber and directed towards the substrate. The plasma reacts with the surface of the substrate to form a thin film of silicon dioxide.\n",
      "\n",
      "3. Sol-gel process: In this method, a sol-gel solution containing silica and other chemicals is applied to a substrate and then heated to form a thin film of silicon dioxide.\n",
      "\n",
      "4. Spray deposition: In this method, a solution containing silica and other chemicals is sprayed onto a substrate and then heated to form a thin film of silicon dioxide.\n",
      "\n",
      "5. Reactive sputtering: In this method, a plasma of silicon and oxygen is generated in a vacuum chamber and directed towards the substrate. The plasma reacts with the surface of the substrate to form a thin film of silicon dioxide.\n"
     ]
    }
   ],
   "source": [
    "print(res.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
