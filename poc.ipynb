{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval augmented generation using `llama_index` with local llm and embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jun/miniforge3/envs/rag/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "llama_model_loader: loaded meta data with 18 key-value pairs and 196 tensors from gpt4all-falcon/gpt4all-falcon-newbpe-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = falcon\n",
      "llama_model_loader: - kv   1:                               general.name str              = Falcon\n",
      "llama_model_loader: - kv   2:                      falcon.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                  falcon.tensor_data_layout str              = jploski\n",
      "llama_model_loader: - kv   4:                    falcon.embedding_length u32              = 4544\n",
      "llama_model_loader: - kv   5:                 falcon.feed_forward_length u32              = 18176\n",
      "llama_model_loader: - kv   6:                         falcon.block_count u32              = 32\n",
      "llama_model_loader: - kv   7:                falcon.attention.head_count u32              = 71\n",
      "llama_model_loader: - kv   8:             falcon.attention.head_count_kv u32              = 1\n",
      "llama_model_loader: - kv   9:        falcon.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,65024]   = [\">>TITLE<<\", \">>ABSTRACT<<\", \">>INTR...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,65024]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,65024]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,64784]   = [\"Ġ t\", \"Ġ a\", \"i n\", \"h e\", \"r e\",...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 11\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   66 tensors\n",
      "llama_model_loader: - type q4_0:  129 tensors\n",
      "llama_model_loader: - type q8_0:    1 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 12/65024 vs 0/65024 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = falcon\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 65024\n",
      "llm_load_print_meta: n_merges         = 64784\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4544\n",
      "llm_load_print_meta: n_head           = 71\n",
      "llm_load_print_meta: n_head_kv        = 1\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 71\n",
      "llm_load_print_meta: n_embd_k_gqa     = 64\n",
      "llm_load_print_meta: n_embd_v_gqa     = 64\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 18176\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 7.22 B\n",
      "llm_load_print_meta: model size       = 3.92 GiB (4.66 BPW) \n",
      "llm_load_print_meta: general.name     = Falcon\n",
      "llm_load_print_meta: BOS token        = 11 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 11 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 138 'Ä'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  3854.98 MiB, ( 3855.05 / 10922.67)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   158.50 MiB\n",
      "llm_load_tensors:      Metal buffer size =  3854.98 MiB\n",
      "...................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3200\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Pro\n",
      "ggml_metal_init: picking default device: Apple M1 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M1 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    25.00 MiB, ( 3881.86 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =    25.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   25.00 MiB, K (f16):   12.50 MiB, V (f16):   12.50 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =   127.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   494.64 MiB, ( 4376.50 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   494.63 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    15.13 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1093\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.eos_token_id': '11', 'tokenizer.ggml.model': 'gpt2', 'falcon.attention.layer_norm_epsilon': '0.000010', 'general.file_type': '2', 'falcon.attention.head_count_kv': '1', 'falcon.attention.head_count': '71', 'falcon.block_count': '32', 'falcon.feed_forward_length': '18176', 'falcon.embedding_length': '4544', 'falcon.tensor_data_layout': 'jploski', 'falcon.context_length': '2048', 'general.name': 'Falcon', 'general.architecture': 'falcon'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# define llm model\n",
    "Settings.llm = LlamaCPP(\n",
    "    model_path=\"gpt4all-falcon/gpt4all-falcon-newbpe-q4_0.gguf\",\n",
    "    context_window=3200,\n",
    "    max_new_tokens=256,\n",
    "    model_kwargs={'n_gpu_layers': -1},\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# define embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"UAE-Large-V1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# use tokenizer from defined llm model\n",
    "Settings.tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"gpt4all-falcon\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# load data and build index\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query your data\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8234.95 ms\n",
      "llama_print_timings:      sample time =       5.99 ms /    28 runs   (    0.21 ms per token,  4677.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2895.62 ms /    11 tokens (  263.24 ms per token,     3.80 tokens per second)\n",
      "llama_print_timings:        eval time =    1291.06 ms /    27 runs   (   47.82 ms per token,    20.91 tokens per second)\n",
      "llama_print_timings:       total time =    4337.16 ms /    38 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Mr Dave Calhoun is leaving Boeing due to the ongoing crisis over the safety of the company's 737 Max planes.\"\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Why is Mr Dave Calhoun leaving Boeing?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is reasonably good and summarizes the what's in the document fed into the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same llm model but without added information, we can ask the same question. The answer is clearly made up with mixed facts here and there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8234.95 ms\n",
      "llama_print_timings:      sample time =      16.87 ms /    67 runs   (    0.25 ms per token,  3972.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     312.09 ms /    10 tokens (   31.21 ms per token,    32.04 tokens per second)\n",
      "llama_print_timings:        eval time =    2078.04 ms /    66 runs   (   31.49 ms per token,    31.76 tokens per second)\n",
      "llama_print_timings:       total time =    2807.86 ms /    76 tokens\n"
     ]
    }
   ],
   "source": [
    "# responce without RAG\n",
    "llm = Settings.llm\n",
    "res = llm.complete(\"Why is Mr Dave Calhoun leaving Boeing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mr. Dave Calhoun is leaving Boeing to pursue other opportunities. He has been with the company for over 30 years and has held various leadership positions, including serving as the CEO of GE Capital and as a member of the board of directors at Boeing. His departure from the company was announced in January 2021.\n"
     ]
    }
   ],
   "source": [
    "print(res.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
